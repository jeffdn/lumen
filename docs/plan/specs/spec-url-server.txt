# $Id$
# ex: set ts=4 noet tw=78:

Summary
----------------------------------------------------------------------------

Component:	URL Server
Summary:	fetches spider candidate urls from doc index, schedules timing
				to be "nice" based on host, and sends urls off to spider.
				maintains queue of any 
Input:		Doc Index "Spider candidates" response (list of URLs)
Output:		URLs scheduled "nicely" and sent to Spider
Issues:		* daemon-type service, runs all the time
			* Can run completely in RAM
			* Must respect vhosts and play nice
			* MUST RESPECT ROBOTS.TXT - TODO!!!

Detailed account of operation
----------------------------------------------------------------------------

Settings:
	Spider_Host = IP/Hostname
		Where the spider's listening connection is
	Spider_Port = INT
		Spider listening port
	Timeout_Spider_Status = INT
		Number of seconds URL Server will wait to be updated on a URL's status
		('WORKING'/'DONE') before assuming a crash and retrying the URL
	Queue_Count = INT
		How many per-host queues should we have?
	Queue_Len = INT
		How large should a single per-host queue be?
	Req_Host_Concurrent = INT
		How many open connections per host to allow
	Req_Host_Per_Sec = FLOAT
		How fast we allow connections to be opened

The URL server maintains a series of per-host queues:

	        |<--- Queue_Len --->| _
			|<-Active->|
	Host(0) [ ][ ][ ][ ][ ]...[ ] ^
	Host(1) [ ][ ][ ][ ][ ]...[ ] |
	Host(1) [ ][ ][ ][ ][ ]...[ ] Queue_Count
	Host(2) [ ][ ][ ][ ][ ]...[ ] |
	...                           v
	Host(n) [ ][ ][ ][ ][ ]...[ ] _

	Note that the host id is not pre-set, when a queue empties, any other host
	can be put in that spot, as long as a per-host queue only represents a
	single host it doesn't matter which.

	Leftover queue:
		Growable array of any documents that:
			* Match a "host" slot but do not fit
			* Do not match a host slot because they are all taken

This is so we can control the rate at which URLs from the same host are
spidered, so we aren't rude and so our spider doesn't get banned. Any URL that
falls within the "Active" range as controlled by Req_Host_Concurrent will be
sent to the Spider.

In order to get around the problem of there being multiple domains running
from the same IP, the urlserver will create two mappings when it receives a
new batch of urls from the document index.  Initially, it will create a
"forward" mapping of domain->IP.  This mapping is cleared every time a new
request to the doc index is made. This mapping will be used to generate the
mapping of IP->urls.  The IP->urls mapping allows the URL server to see how
many URLs share an IP address, to avoid conflict.  If there are more than N
entries for the same IP in the mapping, all the domains on said IP will be
added to the reject list for the next query to the document index.


While at least 1 per-host queue is empty (and no items in the leftover queue
can fill them), produce a request for more URLs from the doc index. Any queue
that is full should be specified in the request's "reject list", so we don't
get any more.

The URL server maintains a network socket connection to the Spider, over which
two-way communication is performed.

The URL Server sends an ID and URL:

<TRANS_ID> ' ' <DOC_ID> ' ' <HOST_IP> ' ' <URL> '\n'

NOTE: TRANS_ID is generated by the URL Server and is only for use in
communication between the URL Server and the Spider. The reason to include
this field is to allow the URL Server to construct an identifier that
makes searching its internal structures faster and also allows for less
complexity inside the URL Server.

When the Spider completes a fetch on a URL (whether or not it is successful)
the Spider sends the following message to the URL Server:

<TRANS_ID> ' ' 'DONE' '\n'

(Yes, I'm using ASCII because URLs are variable length and ASCII aids in
debugging)

If the spider hasn't completed a URL within within n number of seconds,
it will send the following message to the URL server:

<TRANS_ID> ' ' 'WORKING' '\n'

NOTE: To avoid races and near-misses, Timeout_Spider_Status should be a
relatively long time, and significantly longer (probably 2x) than what the
Spider's internal "I should send a status update to the URL server" timer
should be.

Why have the spider send status updates to the URL Server? Why not just have
one-way sending from URL Server -> Spider? For the simple reason that this
allows all queueing and timing to be done by the URL Server and allows the
Spider to simply accept and fetch URLs upon receipt. Because HTTP requests
vary in length from based on the outside network and the server and document
itself, the "DONE" message allows the URL server to throttle it's requests
per host -- it won't send any more requests on a host that has not received
a "DONE" message

The ID will represent 2 pieces of data: the host slot and the active slot.
When the Spider signals that a URL is complete, the URL Server can free that
spot up and shift a URL into it for spidering, while never exceeding the
"nice" rate

If the TCP connection between the URL Server and Spider is ever closed on the
Spider end, the URL Server assumes a Spider crash or restart and will retry
connections to the Spider; upon connecting it will restart any pending URLs.
	
